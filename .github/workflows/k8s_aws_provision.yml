name: EKS - Provision STAGING 🏗️

on: [push]

env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_DEFAULT_REGION: 'eu-north-1'
  ROUTE53_DOMAIN_NAME: callistademo.org
  WORK_DIR: aws/eks-gitops

jobs:
  provision-eks-with-pulumi:
    runs-on: ubuntu-latest
    # env:
    #   PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
    # Create an GitHub environment referencing our EKS cluster endpoint
    environment:
      name: 'aws-eks-staging'
    #   url: ${{ steps.traefik-expose.outputs.traefik_url }}
      # using outputs for providing the Pulumi created kubeconfig to subsequent jobs
      # see https://stackoverflow.com/a/61236803/4964553
      # & see https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idoutputs
    outputs:
      kubeconfig: ${{ steps.pulumi-up.outputs.kubeconfig }}
    steps:
      - name: Checkout 🛎️
        uses: actions/checkout@v3
      
      - name: Setup Node ✨
        uses: actions/setup-node@v3
        with:
          node-version: 16
      
      - name: Configure AWS Credentials 🔐
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-region: ${{ secrets.AWS_REGION }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      # https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows
      - name: Cache node_modules
        uses: actions/cache@v3
        with:
          path: ~/.npm
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install Pulumi dependencies 📦️ 
        run: npm install
        working-directory: ${{ env.WORK_DIR }}
      
      # - name: Install Pulumi CLI
      #   uses: pulumi/action-install-pulumi-cli@v2
      
      - name: Provision Infrastructure 🏗️
        uses: pulumi/actions@v3
        id: pulumi-up
        with:
          command: up
          stack-name: staging # When using an individual account, only use stack-name.
          work-dir: ${{ env.WORK_DIR }}
          refresh: true;
          comment-on-pr: true;
        # run: |
        #   pulumi stack select staging
        #   pulumi preview
        #   echo "lets use --suppress-outputs here in order to prevent Pulumi from logging the kubeconfig into public GitHub Action logs"
        #   pulumi up --yes --suppress-outputs
        #   echo "Create ~/.kube dir only, if not already existent (see https://stackoverflow.com/a/793867/4964553)"
        #   mkdir -p ~/.kube
        #   echo "Create kubeconfig and supply it for depending Action jobs"
        #   pulumi stack output kubeconfig > ~/.kube/config
        #   echo "::set-output name=kubeconfig::$(pulumi stack output kubeconfig)"
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      # - name: Provision AWS EKS cluster with Pulumi
      #   id: pulumi-up
      #   run: |
      #     pulumi stack select staging
      #     pulumi preview
      #     echo "lets use --suppress-outputs here in order to prevent Pulumi from logging the kubeconfig into public GitHub Action logs"
      #     pulumi up --yes --suppress-outputs
      #     echo "Create ~/.kube dir only, if not already existent (see https://stackoverflow.com/a/793867/4964553)"
      #     mkdir -p ~/.kube
      #     echo "Create kubeconfig and supply it for depending Action jobs"
      #     pulumi stack output kubeconfig > ~/.kube/config
      #     echo "::set-output name=kubeconfig::$(pulumi stack output kubeconfig)"
      #   working-directory: ${{ env.WORK_DIR }}

      - name: Try to connect to our EKS cluster using kubectl 🔌
        run: |
          mkdir ~/.kube
          echo '${{ steps.pulumi-up.outputs.kubeconfig }}' > ~/.kube/config
          echo "--- Checking connectivity to cluster"
          kubectl get nodes

      # - name: Install Traefik via Helm
      #   run: |
      #     echo "--- Install Traefik via Helm (which is already installed in GitHub Actions environment https://github.com/actions/virtual-environments)"
      #     helm dependency update traefik/install
      #     helm upgrade -i traefik traefik/install
      # - name: Create or update Route53 hosted zone A record to point to ELB Traefik is configured to
      #   run: |
      #     echo "--- Obtaining the Route53 domain's hosted zone id"
      #     ROUTE53_DOMAIN_HOSTED_ZONE_ID="$(aws route53 list-hosted-zones-by-name | jq --arg name "$ROUTE53_DOMAIN_NAME." -r '.HostedZones | .[] | select(.Name=="\($name)") | .Id')"
      #     echo "--- Obtaining the ELB hosted zone id"
      #     echo "Therefore cutting the ELB url from the traefik k8s Service using cut (see https://stackoverflow.com/a/29903172/4964553)"
      #     ELB_NAME="$(kubectl get service traefik -n default --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}' | cut -d "-" -f 1)"
      #     echo "Extracting the hosted zone it using aws cli and jq (see https://stackoverflow.com/a/53230627/4964553)"
      #     ELB_HOSTED_ZONE_ID="$(aws elb describe-load-balancers | jq --arg name "$ELB_NAME" -r '.LoadBalancerDescriptions | .[] | select(.LoadBalancerName=="\($name)") | .CanonicalHostedZoneNameID')"
      #     echo "--- Obtaining the Elastic Load Balancer url as the A records AliasTarget"
      #     ELB_URL="$(kubectl get service traefik -n default --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
      #     echo "--- Creating or updating ('UPSERT') Route53 hosted zone A record to point to ELB Traefik (see https://aws.amazon.com/premiumsupport/knowledge-center/simple-resource-record-route53-cli/)"
      #     echo "--- Creating Route53 hosted zone record (mind to wrap the variables in double quotes in order to get them evaluated, see https://stackoverflow.com/a/49228748/4964553)"
      #     aws route53 change-resource-record-sets \
      #       --hosted-zone-id $ROUTE53_DOMAIN_HOSTED_ZONE_ID \
      #       --change-batch '
      #       {
      #         "Comment": "Create or update Route53 hosted zone A record to point to ELB Traefik is configured to"
      #         ,"Changes": [{
      #           "Action"              : "UPSERT"
      #           ,"ResourceRecordSet"  : {
      #             "Name"              : "*.'"$ROUTE53_DOMAIN_NAME"'"
      #             ,"Type"             : "A"
      #             ,"AliasTarget": {
      #                 "HostedZoneId": "'"$ELB_HOSTED_ZONE_ID"'",
      #                 "DNSName": "dualstack.'"$ELB_URL"'",
      #                 "EvaluateTargetHealth": true
      #             }
      #           }
      #         }]
      #       }
      #       '
      # - name: Expose Traefik url as GitHub environment
      #   id: traefik-expose
      #   run: |
      #     echo "--- Apply Traefik-ception IngressRule"
      #     kubectl apply -f traefik/traefik-dashboard.yml
      #     echo "--- Wait until Loadbalancer url is present (see https://stackoverflow.com/a/70108500/4964553)"
      #     until kubectl get service/traefik -n default --output=jsonpath='{.status.loadBalancer}' | grep "ingress"; do : ; done
      #     TRAEFIK_URL="http://traefik.$ROUTE53_DOMAIN_NAME"
      #     echo "All Services should be accessible through Traefik Ingress at $TRAEFIK_URL - creating GitHub Environment"
      #     echo "::set-output name=traefik_url::$TRAEFIK_URL"
  install-and-run-argocd-on-eks:
    runs-on: ubuntu-latest
    needs: provision-eks-with-pulumi
    # environment:
    #   name: argocd-dashboard
    #   url: ${{ steps.dashboard-expose.outputs.dashboard_host }}
    steps:
      - name: Checkout 🛎️
        uses: actions/checkout@v3
      # We must use single quotes (!!!) here to access the kubeconfig like this:
      # echo '${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}' > ~/.kube/config
      # Otherwise we'll run into errors like (see https://stackoverflow.com/a/15930393/4964553):
      # "error: error loading config file "/home/runner/.kube/config": yaml: did not find expected ',' or '}'"
      - name: Configure kubeconfig to use with kubectl from provisioning job
        run: |
          mkdir ~/.kube
          echo '${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}'
          echo "${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}"
          echo '${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}' > ~/.kube/config
          echo "--- Checking connectivity to cluster"
          kubectl get nodes
      # - name: Install ArgoCD
      #   run: |
      #     echo "--- Create argo namespace and install it"
      #     kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
      #     echo "--- Install & configure ArgoCD via Kustomize - see https://stackoverflow.com/a/71692892/4964553"
      #     kubectl apply -k argocd/install
      # - name: Expose ArgoCD Dashboard as GitHub environment
      #   id: dashboard-expose
      #   run: |
      #     echo "--- Expose ArgoCD Dashboard via Traefik IngressRoute"
      #     kubectl apply -f traefik/argocd-dashboard.yml
      #     echo "--- Create GitHub environment var"
      #     DASHBOARD_HOST="https://argocd.$ROUTE53_DOMAIN_NAME"
      #     echo "The ArgoCD dashboard is accessible at $DASHBOARD_HOST - creating GitHub Environment"
      #     echo "::set-output name=dashboard_host::$DASHBOARD_HOST"
      # - name: Create GitHub Container Registry Secret to be able to pull from ghcr.io
      #   run: |
      #     echo "--- Create Secret to access GitHub Container Registry"
      #     kubectl create secret docker-registry github-container-registry \
      #         --docker-server=ghcr.io \
      #         --docker-username=${{ secrets.GHCR_USER }} \
      #         --docker-password=${{ secrets.GHCR_PASSWORD }} \
      #         --namespace default \
      #         --save-config --dry-run=client -o yaml | kubectl apply -f -
      # - name: Install ArgoCD CLI
      #   run: brew install argocd

      # - name: Install & configure argocd-task-create-sync-and-wait
      #   run: |
      #     echo "--- Wait until Secret argocd-initial-admin-secret got created for the following argocd login"
      #     until kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data}" | grep "password"; do : ; done
      #     echo "--- Wait until (hopefully) ArgoCD server deployment is ready to be logged in to"
      #     kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd
      #     echo "--- Login argocd CLI - now wrapped in until to prevent dial tcp: lookup 12345.eu-central-1.elb.amazonaws.com on 8.8.8.8:53: no such host (see https://stackoverflow.com/a/71030112/4964553)"
      #     until argocd login argocd.$ROUTE53_DOMAIN_NAME --username admin --password $(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo) --insecure; do : ; done
      #     echo "--- Create ConfigMap to point argocd CLI to our argocd-server"
      #     kubectl create configmap argocd-env-configmap \
      #         --from-literal="ARGOCD_SERVER=argocd.$ROUTE53_DOMAIN_NAME" \
      #         --namespace default \
      #         --save-config --dry-run=client -o yaml | kubectl apply -f -
      #     echo "--- Create AppProject apps2deploy using manifest style incl. role create-sync with needed permissions"
      #     kubectl apply -f argocd/argocd-appproject-apps2deploy.yml
      #     echo "--- Show AppProject details incl. role permissions"
      #     kubectl get appproj -n argocd apps2deploy -o yaml
      #     echo "--- Create Secret for argocd CLI authentication to the argocd-server using AppProject role token"
      #     kubectl create secret generic argocd-env-secret \
      #       --from-literal=ARGOCD_AUTH_TOKEN=$(argocd proj role create-token apps2deploy create-sync --token-only) \
      #       --namespace default \
      #       --save-config --dry-run=client -o yaml | kubectl apply -f -
  # install-and-run-tekton-on-eks:
  #   runs-on: ubuntu-latest
  #   needs: [provision-eks-with-pulumi, install-and-run-argocd-on-eks]
  #   environment:
  #     name: tekton-dashboard
  #     url: ${{ steps.dashboard-expose.outputs.dashboard_host }}
  #   steps:
  #     - name: Checkout
  #       uses: actions/checkout@master
  #     # We must use single quotes (!!!) here to access the kubeconfig like this:
  #     # echo '${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}' > ~/.kube/config
  #     # Otherwise we'll run into errors like (see https://stackoverflow.com/a/15930393/4964553):
  #     # "error: error loading config file "/home/runner/.kube/config": yaml: did not find expected ',' or '}'"
  #     - name: Configure kubeconfig to use with kubectl from provisioning job
  #       run: |
  #         mkdir ~/.kube
  #         echo '${{ needs.provision-eks-with-pulumi.outputs.kubeconfig }}' > ~/.kube/config
  #         echo "--- Checking connectivity to cluster"
  #         kubectl get nodes

      